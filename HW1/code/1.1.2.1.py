# -*- coding: utf-8 -*-
"""cnn_hidden1_dim16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zDnYNNh7TufDvBcvjz5s3j3C5cbRfJcY
"""

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

tf.VERSION

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.examples.tutorials.mnist import input_data
data = input_data.read_data_sets('data/MNIST/', one_hot=True)
data.test.cls = np.argmax(data.test.labels, axis=1)

img_size=28
img_size_flat=img_size*img_size
img_shape=(img_size,img_size)
num_channels=1
num_classes=10

x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')
x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])
y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')
y_true_cls = tf.argmax(y_true, dimension=1)

# layer_conv1
net = tf.layers.conv2d(inputs=x_image, name='layer_conv1', padding='same',
                       filters=16, kernel_size=5, activation=tf.nn.relu)
net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)


net = tf.layers.flatten(net)

net = tf.layers.dense(inputs=net,name='layer_fc1',units=128, activation=tf.nn.relu)
logits=tf.layers.dense(inputs=net,name='layer_fc_out',units=num_classes)

y_pred = tf.nn.softmax(logits=logits)
y_pred_cls = tf.argmax(y_pred, dimension=1)

cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=logits)
loss = tf.reduce_mean(cross_entropy)

opt = tf.train.AdamOptimizer(learning_rate=1e-4)
optimizer = opt.minimize(loss)

correct_prediction = tf.equal(y_pred_cls, y_true_cls)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

trainable_var_list = tf.trainable_variables()

def get_weights_variable(layer_name):

    with tf.variable_scope(layer_name, reuse=True):
        variable = tf.get_variable('kernel')

    return variable

weights_conv1 = get_weights_variable(layer_name='layer_conv1')

weights_fc1 = get_weights_variable(layer_name='layer_fc1')
weights_fc_out = get_weights_variable(layer_name='layer_fc_out')

session = tf.Session()
session.run(tf.global_variables_initializer())
grads = tf.gradients(loss, weights_fc_out)[0]
hessian = tf.reduce_sum(tf.hessians(loss, weights_fc_out)[0], axis = 2)

data = input_data.read_data_sets('data/MNIST/', one_hot=True)
data.test.cls = np.argmax(data.test.labels, axis=1)
loss_list=[]
accuracy_training_list=[]
accuracy_testing_list=[]

epoch_times=100
iteration_times=100
batch_number=int(55000/iteration_times)
for epoch in range(0,epoch_times):
    for iteration in range(0,iteration_times):
        x_batch, y_true_batch = data.train.next_batch(batch_number)
        feed_dict_train = {x: x_batch,
                   y_true: y_true_batch}
        session.run(optimizer, feed_dict=feed_dict_train)
        los, acc = session.run([loss, accuracy], feed_dict=feed_dict_train)
        
    loss_list.append(los)
    accuracy_training_list.append(acc)
   
    num_test = len(data.test.images)
    cls_pred = np.zeros(shape=num_test, dtype=np.int)
    i = 0
    while i < num_test:
        j =  num_test
        images = data.test.images[i:j, :]
        labels = data.test.labels[i:j, :]
        feed_dict = {x: images,
                     y_true: labels}
        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)
        i = j
        cls_true = data.test.cls
        correct = (cls_true == cls_pred)
        correct_sum = correct.sum()
        acc = float(correct_sum) / num_test
    accuracy_testing_list.append(acc)

import matplotlib.pyplot as plt
plt.plot(loss_list)
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
#plt.gca().set_yscale('log')
plt.show()


plt.plot(accuracy_training_list)
plt.plot(accuracy_testing_list)
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
#plt.gca().set_yscale('log')
plt.show()

data = input_data.read_data_sets('data/MNIST/', one_hot=True)
data.test.cls = np.argmax(data.test.labels, axis=1)

img_size=28
img_size_flat=img_size*img_size
img_shape=(img_size,img_size)
num_channels=1
num_classes=10

x = tf.placeholder(tf.float32, shape=[None, img_size_flat], name='x')
x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])
y_true = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_true')
y_true_cls = tf.argmax(y_true, dimension=1)

# layer_conv1
net = tf.layers.conv2d(inputs=x_image, name='layer_conv21', padding='same',
                       filters=16, kernel_size=5, activation=tf.nn.relu)
net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)
net = tf.layers.conv2d(inputs=net, name='layer_conv22', padding='same',
                       filters=36, kernel_size=5, activation=tf.nn.relu)
net = tf.layers.max_pooling2d(inputs=net, pool_size=2, strides=2)


net = tf.layers.flatten(net)

net = tf.layers.dense(inputs=net,name='layer_fc21',units=128, activation=tf.nn.relu)
logits=tf.layers.dense(inputs=net,name='layer_fc_out2',units=num_classes)

y_pred = tf.nn.softmax(logits=logits)
y_pred_cls = tf.argmax(y_pred, dimension=1)

cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=logits)
loss = tf.reduce_mean(cross_entropy)

opt = tf.train.AdamOptimizer(learning_rate=1e-4)
optimizer = opt.minimize(loss)

correct_prediction = tf.equal(y_pred_cls, y_true_cls)
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

trainable_var_list = tf.trainable_variables()

def get_weights_variable(layer_name):

    with tf.variable_scope(layer_name, reuse=True):
        variable = tf.get_variable('kernel')

    return variable

weights_conv1 = get_weights_variable(layer_name='layer_conv21')
weights_conv2 = get_weights_variable(layer_name='layer_conv22')

weights_fc1 = get_weights_variable(layer_name='layer_fc21')
weights_fc_out = get_weights_variable(layer_name='layer_fc_out2')

session = tf.Session()
session.run(tf.global_variables_initializer())
grads = tf.gradients(loss, weights_fc_out)[0]
hessian = tf.reduce_sum(tf.hessians(loss, weights_fc_out)[0], axis = 2)

data = input_data.read_data_sets('data/MNIST/', one_hot=True)
data.test.cls = np.argmax(data.test.labels, axis=1)
loss_list2=[]
accuracy_training_list2=[]
accuracy_testing_list2=[]

epoch_times=100
iteration_times=100
batch_number=int(55000/iteration_times)
for epoch in range(0,epoch_times):
    for iteration in range(0,iteration_times):
        x_batch, y_true_batch = data.train.next_batch(batch_number)
        feed_dict_train = {x: x_batch,
                   y_true: y_true_batch}
        session.run(optimizer, feed_dict=feed_dict_train)
        los, acc = session.run([loss, accuracy], feed_dict=feed_dict_train)
        
    loss_list2.append(los)
    accuracy_training_list2.append(acc)
   
    num_test = len(data.test.images)
    cls_pred = np.zeros(shape=num_test, dtype=np.int)
    i = 0
    while i < num_test:
        j =  num_test
        images = data.test.images[i:j, :]
        labels = data.test.labels[i:j, :]
        feed_dict = {x: images,
                     y_true: labels}
        cls_pred[i:j] = session.run(y_pred_cls, feed_dict=feed_dict)
        i = j
        cls_true = data.test.cls
        correct = (cls_true == cls_pred)
        correct_sum = correct.sum()
        acc = float(correct_sum) / num_test
    accuracy_testing_list2.append(acc)

plt.plot(loss_list2)
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
#plt.gca().set_yscale('log')
plt.show()


plt.plot(accuracy_training_list2)
plt.plot(accuracy_testing_list2)
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
#plt.gca().set_yscale('log')
plt.show()

plt.plot(loss_list)
plt.plot(loss_list2)
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
#plt.gca().set_yscale('log')
plt.show()

plt.plot(accuracy_training_list)
plt.plot(accuracy_testing_list)
plt.plot(accuracy_training_list2)
plt.plot(accuracy_testing_list2)
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
#plt.gca().set_yscale('log')
plt.show()